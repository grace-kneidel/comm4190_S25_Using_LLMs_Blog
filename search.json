[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "AI as a Realtor\n\n\n\n\n\n\nprompting\n\n\n\nCan LLMs help me figure out which Philadelphia neighborhood to move to? \n\n\n\n\n\nFeb 23, 2025\n\n\nGrace\n\n\n\n\n\n\n\n\n\n\n\n\nSVG representations of economists\n\n\n\n\n\n\nSVGs\n\n\n\n\n\n\n\n\n\nFeb 22, 2025\n\n\nGrace\n\n\n\n\n\n\n\n\n\n\n\n\nTable transcription test\n\n\n\n\n\n\ntranscription\n\n\nhistorical data\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\nGrace\n\n\n\n\n\n\n\n\n\n\n\n\nHow well can AI move from one coding language to another?\n\n\n\n\n\n\nStata\n\n\nR\n\n\ntranslation\n\n\n\nIt seems to depend a lot on the language\n\n\n\n\n\nFeb 15, 2025\n\n\nGrace\n\n\n\n\n\n\n\n\n\n\n\n\nThe environmental impact of generative AI\n\n\n\n\n\n\nprompting\n\n\ncomparing models\n\n\nenvironment\n\n\n\nDo LLMs know how much water they use?\n\n\n\n\n\nFeb 7, 2025\n\n\nGrace\n\n\n\n\n\n\n\n\n\n\n\n\nCan ChatGPT help me open the correct PDF?\n\n\n\n\n\n\nprompting\n\n\n\nIt turns out to be quite the challenge.\n\n\n\n\n\nFeb 1, 2025\n\n\nGrace\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/004_read_housing_data.html",
    "href": "posts/004_read_housing_data.html",
    "title": "Table transcription test",
    "section": "",
    "text": "Introduction\nUsing historical data is pretty fun, but it often involves some grunt work. Sometimes data isn’t available in a tabular format and needs to be manually transcribed so that it is ready for analysis. Today, I’m going to build off my first blog post and see if LLMs are able to transcribe values of a table for me.\nI’m going to continue working with the 1950 Census of Housing, linked here. Some of the scans are very easy for me to read, and others are fuzzier. Let’s see if GPT-4o is up for the test!\n\nWarm-up exercise: an easy example\nLet’s see if ChatGPT can read this clear scan. \n\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\n\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\nclient = OpenAI()\n\n\n\nprompt = 'Describe this image of a table. Suppose you needed to transcribe the data into a .csv file. How many columns would the .csv file have, and what would their names be? How many rows would there be? Think it though. Then, transcribe the data into a semicolon delimited file' \n\nbase64_image = encode_image('img/img1.png')\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n)\n\n\nprint(response.choices[0].message.content)\n\nTo transcribe the data into a .csv file, you first need to understand the structure of the table. Here's the breakdown:\n\n### Columns\nThere are 13 columns in the table with the following headers:\n\n1. Subject\n2. Total occupied: Total\n3. Total occupied: 1 dwelling unit, detached\n4. Total occupied: Other 1, and 2 dwelling unit\n5. Total occupied: 3 and 4 dwelling unit\n6. Total occupied: 5 to 9 dwelling unit\n7. Total occupied: 10 dwelling unit or more\n8. Owner occupied: Total\n9. Owner occupied: 1 dwelling unit, detached\n10. Owner occupied: All other dwelling units\n11. Renter occupied: Total\n12. Renter occupied: 1 dwelling unit, detached\n13. Renter occupied: Other 1, and 2 dwelling unit\n14. Renter occupied: 3 and 4 dwelling unit\n15. Renter occupied: 5 to 9 dwelling unit\n16. Renter occupied: 10 dwelling unit or more\n\n### Rows\nThere are six rows in total:\n\n1. Header row\n2. All occupied units\n3. 1945 or later\n4. 1940 to 1944\n5. 1939 or earlier\n6. Not reported\n\nNow, let's transcribe the data into a semicolon-delimited format:\n\n```plaintext\nSubject;Total occupied: Total;Total occupied: 1 dwelling unit, detached;Total occupied: Other 1, and 2 dwelling unit;Total occupied: 3 and 4 dwelling unit;Total occupied: 5 to 9 dwelling unit;Total occupied: 10 dwelling unit or more;Owner occupied: Total;Owner occupied: 1 dwelling unit, detached;Owner occupied: All other dwelling units;Renter occupied: Total;Renter occupied: 1 dwelling unit, detached;Renter occupied: Other 1, and 2 dwelling unit;Renter occupied: 3 and 4 dwelling unit;Renter occupied: 5 to 9 dwelling unit;Renter occupied: 10 dwelling unit or more\nAll occupied units;232,265;167,920;31,160;16,225;11,055;5,905;128,110;117,365;10,745;104,155;50,555;24,225;14,940;8,680;5,755\n1945 or later;62,332;49,792;5,257;3,300;2,277;1,706;42,380;40,520;1,860;19,952;9,272;4,040;3,131;1,802;1,706\n1940 to 1944;30,974;25,940;1,902;357;1,580;1,195;22,292;21,252;1,040;8,682;4,688;1,464;357;1,053;1,120\n1939 or earlier;129,422;86,542;22,032;11,457;6,672;2,719;60,170;53,814;7,356;69,252;32,728;17,084;10,409;5,379;3,682\nNot reported;9,529;5,649;1,967;1,111;522;280;3,270;2,784;486;6,259;2,865;1,627;1,044;443;280\n```\n\nEnsure each value is separated by a semicolon, and commas within numbers are preserved as shown.\n\n\nI’m pretty impressed with the output, but it’s not quite good enough to be able to trust 100% without double checking the numbers. I stared at the output for a little bit before realizing that one of the values in the bottom right of the pdf, 2,648, was transcribed as 3,682.\n\n\n\nChatGPT’s output\n\n\n\n\n\nThe “3,682” in question\n\n\nThis is a little disheartening! Just to be sure, I’m going to ask ChatGPT to read only this one number specifically to see if the results improve. Maybe the size of the table it needed to transcribe seemed overwhelming, or the prompt I gave it was too long.\n\nprompt = 'What number is this?' \n\nbase64_image = encode_image('img/img3.png')\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o\",\n    n=3,\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n)\n\nresponse = completion.choices[0].message.content\n\n\nfor rnum, resp in enumerate(completion.choices,1): \n    display((f'Response {rnum}: {resp.message.content}'))\n\n'Response 1: The number is 2,648.'\n\n\n'Response 2: The number is 2,648.'\n\n\n'Response 3: The number is 2,648.'\n\n\nYes, ChatGPT, that’s correct! So maybe the issue was that I assigned too big of a task, making ChatGPT get “tired”.\n\n\nThe test\nGiven that my intended warm-up round was not exactly a walk in the park, I’m going to make sure to not give ChatGPT too many numbers to transcribe in one prompt. However, I do want to see how it handles fuzzy text. Let’s try with this image now.\n\nThe row names are pretty fuzzy, so I’m excited to see what the LLM is able to make out.\n\nprompt = 'Describe this image of a table. Suppose you needed to transcribe the data into a .csv file. How many columns would the .csv file have, and what would their names be? How many rows would there be? Think it though. Then, transcribe the data into a semicolon delimited file' \n\nbase64_image = encode_image('img/img4.png')\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n)\n\n\nprint(response.choices[0].message.content)\n\nTo transcribe the table into a .csv file, we first identify the columns and rows.\n\n### Columns\n\nThere are 7 columns in the table:\n1. Subject\n2. Total\n3. 1 dwelling unit, detached\n4. Other 1, and 2 dwelling unit\n5. 3 and 4 dwelling unit\n6. 5 to 9 dwelling unit\n7. 10 dwelling unit or more\n\n### Rows\n\nThere are 5 rows including the header row:\n1. Header\n2. All occupied units\n3. Year built: 1940 or earlier\n4. Year built: 1940 to 1949\n5. Year built: 1950 or later\n6. Year built: Not reported\n\n### Transcription to a Semicolon Delimited File\n\nHere's how the data would look in a semicolon-delimited format:\n\n```\nSubject;Total;1 dwelling unit, detached;Other 1, and 2 dwelling unit;3 and 4 dwelling unit;5 to 9 dwelling unit;10 dwelling unit or more\nAll occupied units;35,460;23,210;5,320;2,375;1,685;450\n1940 or earlier;13,426;11,063;1,196;473;593;101\n1940 to 1949;4,737;4,351;251;73;29;27\n1950 or later;15,230;12,212;3,678;1,744;1,004;242\nNot reported;1,599;1,185;195;79;59;81\n```\n\nThis setup ensures the data is organized and easy to import into a program that can process .csv files.\n\n\n\n\n\nNot quite\n\n\nThe screenshot I provided was admittedly hard to read, but I thought ChatGPT would be able to make out the difference between the blurry “earlier” and the blurry “later”. Let’s see what happens if I focus its attention on these words.\n\n\nprompt = 'There are 4 lines of text under the header \"YEAR BUILT\" in this image. What do they say? Reply only with your transcriptions of the text below the header \"YEAR BUILT\". Separate each of the 4 lines with quotations' \n\nbase64_image = encode_image('img/img6.png')\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o\",\n    n=3,\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n)\n\nresponse = completion.choices[0].message.content\n\n\nfor rnum, resp in enumerate(completion.choices,1): \n    display((f'Response {rnum}: {resp.message.content}'))\n\n'Response 1: \"1949 or later\" \"1939 to 1948\" \"1938 or earlier\" \"Not reported\"'\n\n\n'Response 2: \"1980 or later\" \"1940 to 1979\" \"1939 or earlier\" \"Not reported\"'\n\n\n'Response 3: \"1949 or later\"  \\n\"1940 to 1949\"  \\n\"1939 or earlier\"  \\n\"Not reported\"'\n\n\nThis seems to just be a lot harder than I thought it would be. At least with its attention focused, it can consistently identify the word “later” vs the word “earlier”, which it was not able to do before. It’s impressive that the categories make sense looking at one response at a time, at least for the first 2 responses- there is no overlap between the year ranges, and all years are covered. This indicates some level of logical reasoning. The third response would be double counting 1949 data in two of its categories.\nFinally, I’m curious to see if o1 will be any better than GPT-4o at this.\n\nprompt = 'There are 4 lines of text under the header \"YEAR BUILT\" in this image. What do they say? Reply only with your transcriptions of the text below the header \"YEAR BUILT\". Separate each of the 4 lines with quotations' \n\nbase64_image = encode_image('img/img6.png')\n\ncompletion = client.chat.completions.create(\n    model=\"o1\",\n    n=3,\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n)\n\nresponse = completion.choices[0].message.content\n\n\nfor rnum, resp in enumerate(completion.choices,1): \n    display((f'Response {rnum}: {resp.message.content}'))\n\n'Response 1: \"1961 or later\"  \\n\"1939 to 1941\"  \\n\"1939 or earlier\"  \\n\"Not reported\"'\n\n\n'Response 2: \"1949 or later\"  \\n\"1939 to 1944\"  \\n\"1939 or earlier\"  \\n\"Not reported\"  '\n\n\n'Response 3: \"1939 or later\"  \\n\"1939 to 1941\"  \\n\"1939 or earlier\"  \\n\"Not reported\"  '\n\n\nI wish I had timed it, but o1 took MUCH longer to run than GPT-4o. Response 2 here is the closest we have gotten so far, but the first line should say 1945. It’s crazy that all 3 responses got the third line correctly. Well, looks like humans are going to be transcribing historical data for at least a little while longer."
  },
  {
    "objectID": "posts/005_economist_svg.html",
    "href": "posts/005_economist_svg.html",
    "title": "SVG representations of economists",
    "section": "",
    "text": "Today, I’m excited to experiment with generative AI’s ability to write code for SVGs.\n\nThe plan\n\nAsk Claude to make me SVGs of economists\nAsk GPT-4o what job the cartoon in the SVG depicts\nAsk GPT-4o if it thinks the cartoon is a man or woman\nAsk Claude to make me cartoons of economists who are women\n\n\n1. Ask Claude to make me SVGs of economists\nI prompted Claude “Please make code for an svg of an economist”, intentionally keeping the prompt vague to see what Claude thought economists looked like. I then regenerated the response 4 times. I found the responses to be pretty funny. Here’s an example of the prompt and response. \nAnd here’s our full lineup of SVG economists:\n\n\n\nEconomist 1!\n\n\n\n\n\nEconomist 2!\n\n\n\n\n\nEconomist 3!\n\n\n\n\n\nEconomist 4!\n\n\n\n\n\nEconomist 5!\n\n\nMy main takeaways:\n\nFor simple SVG images, these images are pretty complex. Claude found lots of creative ways to refer to the use of data and economic indicators.\nClaude does not seem to be very good at making SVGs where hair is properly placed on one’s head. Economist 4’s pointers are also pretty funny.\nThese all look pretty masculine to me.\n\n\n\n2. Ask GPT-4o what job the cartoon in the SVG depicts\nNext, I was curious which cartoon GPT-4o would be able to most easily identify as an economist. I asked GPT-4o first “What does this image depict?”, and then “What job do you think the cartoon figure has?”. I used a fresh window for each SVG.\n\n\n\n\n\n\n\nEconomist SVG\nJob GPT-4o Thinks It Has\n\n\n\n\n1\n“The cartoon figure is likely representing a business person or analyst, given the suit and the items resembling a graph and a report, which suggest involvement in data analysis or presentations.”\n\n\n2\n“The cartoon figure likely represents a financial professional, such as an economist, banker, or stockbroker, given the presence of currency symbols and the graph-like line.”\n\n\n3\nThe cartoon figure likely represents someone in a financial or analytical role, such as an accountant, financial analyst, or business consultant, given the chart and calculator they are holding.\n\n\n4\nThe cartoon figure likely represents a business professional or investor, given the suit, tie, glasses, and the financial symbols like the dollar sign, bar chart, and rising line graph.\n\n\n5\nThe cartoon figure likely represents an economist or a financial analyst, given the focus on GDP and the business attire.\n\n\n\nMy main takeaways:\n\nChatGPT did a pretty good job with all of these. All the proposed jobs seem reasonable.\nEconomic terminology like GDP and currency symbols indicated being an economist the most.\n\nI agree with ChatGPT that SVGs 2 and 5 look especially like economists.\n\n\n3. Ask GPT-4o if it thinks the cartoon is a man or woman\nNow I prompted: “Do you think the cartoon is of a A) man or B) woman? YOU MUST ANSWER A OR B. Why? How confident do you feel about this on a scale of 1 to 10?”. Here are the results.\n\n\n\nEconomist SVG\nMan or woman?\nConfidence\n\n\n\n\n1\nMan\n6\n\n\n2\nMan\n7\n\n\n3\nMan\n7\n\n\n4\nMan\n4\n\n\n5\nMan\n3\n\n\n\nI thought these results were interesting because I perceived SVGs 1-4 to be about equally representative of a man.\n\n\n4. Ask Claude to make me cartoons of economists who are women\nSo, what do women economists look like according to Claude? I went right back to my original chat window and replied by asking for a picture of a woman economist. This is what I got:\n\n\n\nA woman!\n\n\nI find it pretty hilarious to compare the “key changes” to the SVG itself. I see no tailoring on this blazer, and no differences in facial features!\nIn response, I said “These still look like men, there are economists who are women!”.\nBut Claude interpreted that as an attack of sorts, and started apologizing for stereotyping women.\nYou’re absolutely right - my SVG attempts weren’t doing a good job representing women economists realistically. Women economists come from all backgrounds and have many different appearances and styles, just like any other profession. I should have:\n\nAvoided relying on stereotypical “feminine” markers like skirts and necklaces\nRecognized that professional women don’t need to be drawn differently than professional men in many cases\nUnderstood that simple additions like longer hair or accessories don’t automatically make a good representation of a woman\n\nAn interesting response, since I see no necklace in this SVG! Here’s Claude’s second attempt.\nKey improvements in this version: 1. More natural, professional appearance without relying on gender stereotypes 2. Professional suit in a modern cut that works for any gender 3. Natural hairstyle that could represent various hair types and textures 4. Focus on the professional elements (economic analysis, research) rather than decorative elements 5. Realistic skin tone that could represent diverse backgrounds 6. Clean, modern professional attire that focuses on the role rather than gender\nThe emphasis is on depicting a competent economist who happens to be a woman, rather than trying to “feminize” the figure with stereotypical elements. The character maintains all the professional elements that represent the field of economics while being more inclusive and realistic.\n\n\n\nAnother woman!\n\n\nSo now a characteristic of being an economist is wearing glasses? Also, the necklace remains invisible! And the skin color has barely changed!\n\n\nConclusion\nI’ll admit that SVGs are simple graphics that don’t have the capacity for a great amount of detail. Still, I’m almost by impressed by how bad Claude is at making SVGs of humans that do not have hair covering their eyes! There seems to be several big gaps here.\nGap 1: Claude will say it is graphing something when it wrote no code to draw it (the necklace situation).\nGap 2: Claude thinks that some of the things it drew appear differently than they actually do. For example, the white blouse detail does not look like much of a white blouse! Maybe this is an overestimation of how much detail it thinks it can include in an SVG.\nGap 3: Claude is not automatically assessing the SVG output as a quality check and asking “does this look like a human?” (the hair over face situation!).\nOverall, this was a fun exercise and refreshing to do after working with DALL-E 3, which doesn’t seem to be able to keep images simple no matter how hard I try."
  },
  {
    "objectID": "posts/002_water.html",
    "href": "posts/002_water.html",
    "title": "The environmental impact of generative AI",
    "section": "",
    "text": "Lots of people, myself included, have some concerns about using LLMs because they are so resource intensive. Today, I figured I would test AI’s own awareness of its water needs and compare them to the current estimates. I’m going to vary this across different models that have different knowledge cutoffs to see how the responses vary. New work on the water use of generative AI could have happened since these models were trained, so I’m hoping we will see some cool variation."
  },
  {
    "objectID": "posts/002_water.html#the-test",
    "href": "posts/002_water.html#the-test",
    "title": "The environmental impact of generative AI",
    "section": "The test",
    "text": "The test\n\nPart 1: gpt-4o\n\n\nShow the code\nfrom openai import OpenAI\nimport os\nimport pandas as pd\n\nfrom IPython.display import Markdown, display\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\n\n\n\n\nShow the code\nclient = OpenAI()\n\nmodel = \"gpt-4o\"\nsystem_role = \"You are a reporter for The Economist covering environmental economics. Give your responses in Markdown format.\"\ninitial_prompt = '''First, state your knowledge cutoff. Then, write 1 paragraph explaining the environmental impact of each LLM query. Focus on the use \nper single query. Compare to other water intensive activities such as brushing your teeth to give context. Be sure to \nuse the most recent scientific estimates and cite your sources.'''\n\nmessages = [\n    {\"role\": \"system\", \"content\": system_role} ,\n    {\"role\": \"user\", \"content\": initial_prompt},\n    # {\"role\": \"assistant\", \"content\": \"\" },\n    # {\"role\": \"user\", \"content\": \"\" }\n]\n\ncompletion = client.chat.completions.create(\n  model=model,        # the model to use \n  messages=messages,  # a list of dictionaries with the prompt\n  n=3,                # how many responses to generate\n  temperature=0.7     # temperature parameter\n)\n\nresponse = completion.choices[0].message.content\n\n\n\n\nShow the code\nfor rnum, resp in enumerate(completion.choices,1): \n    display(Markdown(f'-----\\n#### Response {rnum} \\n {resp.message.content} \\n'))\n\n\n\n\nResponse 1\nKnowledge Cutoff: October 2023\nThe environmental impact of each large language model (LLM) query, specifically in terms of water usage, can be relatively small when compared to other daily activities. According to recent estimates, processing a single LLM query can consume approximately 0.001 liters of water, accounting for the water used in cooling data centers and powering the necessary computing infrastructure (Patterson et al., 2023). In contrast, activities like brushing your teeth with the tap running can use around 7.5 to 15 liters of water per session (U.S. EPA). This comparison indicates that a single LLM query has a significantly lower water footprint than such common water-intensive activities. Nonetheless, as the use of LLMs scales globally, their cumulative environmental impact becomes more pronounced. This underscores the importance of optimizing data center efficiency and exploring sustainable practices in AI development to mitigate broader environmental effects.\nSources: - Patterson, D., et al. (2023). “The Carbon Footprint of Machine Learning.” Proceedings of the Conference on Machine Learning and Climate Change. - U.S. Environmental Protection Agency (EPA). “WaterSense: Brushing Your Teeth.” EPA.gov\n\n\n\n\n\nResponse 2\nKnowledge Cutoff: October 2023\nThe environmental impact of a single Large Language Model (LLM) query, such as those processed by cutting-edge AI systems, primarily revolves around energy consumption and associated carbon emissions rather than direct water usage. The energy required for a single LLM query is typically measured in terms of kilowatt-hours (kWh), which indirectly relates to water usage through the cooling processes of data centers and energy production. According to recent estimates, each LLM query consumes approximately 0.0003 kWh of energy (Strubell et al., 2019). In comparison, brushing your teeth with water running for two minutes uses about 4 gallons (15.1 liters) of water (EPA, 2023). While the exact water footprint of a single LLM query is difficult to quantify in isolation due to variations in data center efficiencies and energy grids, it is substantially less direct than the tangible water usage involved in daily activities like tooth brushing. However, as LLM utilization scales globally, the cumulative environmental impact becomes significant, prompting calls for more sustainable AI practices and energy sources.\n\n\n\n\n\nResponse 3\nKnowledge Cutoff: October 2023\nThe environmental impact of each Large Language Model (LLM) query is primarily associated with the energy consumption required to power the data centers that host these models. Recent estimates suggest that a single query to an LLM consumes approximately 0.0003 kWh of energy, which translates to a very small carbon footprint. In terms of water usage, this energy consumption indirectly reflects water use, as water is often used for cooling data centers. According to a study by Strubell et al. (2019), the water footprint for a single AI query is quite minimal, comparable to a few milliliters of water. To provide context, brushing your teeth with the tap running can use around 7.5 liters of water per minute. Thus, the water footprint of an LLM query is vastly smaller. However, when scaled up to millions of queries, the cumulative impact becomes significant, highlighting the importance of developing more energy-efficient algorithms and infrastructure (Strubell et al., 2019; Google AI, 2021).\nSources:\n\nStrubell, E., Ganesh, A., & McCallum, A. (2019). Energy and Policy Considerations for Deep Learning in NLP. arXiv preprint arXiv:1906.02243.\nGoogle AI Blog (2021). Our commitment to sustainability through AI. Link\n\n\n\n\nWow! These estimates vary a lot, from 0.5 liters to 0.0005 liters.\n\n\nPart 2: gpt-3.5-turbo\n\n\nShow the code\nmodel = \"gpt-3.5-turbo\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": system_role} ,\n    {\"role\": \"user\", \"content\": initial_prompt},\n    # {\"role\": \"assistant\", \"content\": \"\" },\n    # {\"role\": \"user\", \"content\": \"\" }\n]\n\n\n\ncompletion = client.chat.completions.create(\n  model=model,        # the model to use \n  messages=messages,  # a list of dictionaries with the prompt\n  n=3,                # how many responses to generate\n  temperature=0.7     # temperature parameter\n)\n\nresponse = completion.choices[0].message.content\n\n\n\n\nShow the code\nfor rnum, resp in enumerate(completion.choices,1): \n    display(Markdown(f'-----\\n#### Response {rnum} \\n {resp.message.content} \\n'))\n\n\n\n\nResponse 1\nKnowledge cutoff: October 2021\n\nQuery: “How much water is used to produce one pound of beef?”\n\nThe environmental impact of producing one pound of beef is significant in terms of water usage. According to recent studies, it is estimated that approximately 1,799 gallons of water are required to produce one pound of beef, taking into account water used for feed production, drinking water for the animals, and processing. This high water usage is mainly attributed to the water-intensive nature of growing feed crops for livestock and providing water for the animals throughout their life cycle. To put this into perspective, the water used to produce one pound of beef is roughly equivalent to the amount of water an average person would use for brushing their teeth daily for over two and a half years.\nSources: - Mekonnen, M.M. and Hoekstra, A.Y. (2012). A Global Assessment of the Water Footprint of Farm Animal Products. Ecosystems, 15(3), 401-415.\n\nQuery: “How much water is used to produce one cotton t-shirt?”\n\nThe environmental impact of producing one cotton t-shirt in terms of water usage is substantial. Recent data suggests that approximately 713 gallons of water are needed to produce one cotton t-shirt, accounting for water used in growing cotton plants, processing the fibers, and dyeing the fabric. The water footprint of cotton production is particularly high due to the crop’s demand for irrigation and the water-intensive processes involved in turning raw cotton into a finished garment. To provide context, the water used to make one cotton t-shirt is equivalent to the amount of water an individual would use for brushing their teeth daily for almost 4 years.\nSources: - Mekonnen, M.M. and Hoekstra, A.Y. (2011). The Green, Blue and Grey Water Footprint of Crops and Derived Crop Products. Hydrology and Earth System Sciences, 15(5), 1577-1600.\n\n\n\n\n\nResponse 2\nKnowledge cutoff: September 2021\n\nLLM query: “How much water is used to produce one almond?”\nThe environmental impact of producing one almond is significant due to the high water intensity of almond cultivation. According to a study by the University of California, it takes approximately 1.1 gallons of water to produce a single almond. This is primarily because almond trees require a substantial amount of water to grow, especially in regions with arid climates where they are predominantly cultivated. To put this into context, producing one almond is equivalent to the amount of water used by brushing your teeth for about 10 minutes. The water footprint of almonds has raised concerns about the sustainability of almond farming, particularly in water-stressed regions like California where a large percentage of the world’s almonds are grown.\nLLM query: “How much water is used to produce one avocado?”\nThe environmental impact of producing one avocado is also notable in terms of water usage. According to research published in the journal Water, it takes around 72 gallons of water to produce a single avocado. Avocado trees are known to be water-intensive crops, requiring significant irrigation to support their growth and yield. To put this in perspective, the water footprint of one avocado is equivalent to the amount of water used by brushing your teeth for approximately 170 times. This highlights the substantial water footprint of avocados and underscores the importance of considering the environmental impact of consuming water-intensive foods like avocados, especially in regions facing water scarcity challenges.\nLLM query: “How much water is used to produce one pound of beef?”\nThe environmental impact of producing one pound of beef is notably high in terms of water consumption. According to a report by the Water Footprint Network, it takes approximately 1,800 gallons of water to produce one pound of beef. This is primarily due to the water needed for livestock drinking, irrigation of animal feed crops, and meat processing. To provide context, the water footprint of one pound of beef is equivalent to the amount of water used by brushing your teeth for about 3,600 times. The substantial water footprint of beef production has raised concerns about the sustainability of meat consumption, particularly as global demand for animal products continues to rise and water resources become increasingly strained.\n\n\n\n\n\n\n\n\n\n\n#### Response 3 Knowledge cutoff: September 2021\n\n\n\n\n#### Response 1 My knowledge cutoff is October 2023.\n\n\nRecent studies indicate that while the training phase of large language models (LLMs) is highly energy intensive, the inference phase—i.e. processing a single query—generally consumes only a few milli‐kilowatt‐hours, resulting in an extremely modest direct water footprint when considering data center cooling. For instance, estimates suggest that generating 1 kWh of electricity in thermoelectric plants may involve roughly 2–3 liters of water (U.S. Environmental Protection Agency, 2019)[1]; thus, an inference operation consuming around 0.001 kWh might indirectly “use” only about 0.002–0.003 liters of water. In comparison, a typical tooth brushing episode may use 2–3 liters of water (Hoekstra et al., 2011)[2], which puts the water intensity of a single LLM query into stark perspective. Although the cumulative environmental impact of millions of such queries should not be overlooked, on an individual basis the direct water usage is negligible relative to everyday activities like brushing your teeth.\n\n\n\n[1] U.S. Environmental Protection Agency. (2019). Water Use in Electricity Production.\n[2] Hoekstra, A. Y., et al. (2011). The Water Footprint Assessment Manual. UNESCO.\n\n\n\n\nResponse 2\nKnowledge Cutoff: October 2023\nRecent studies indicate that while the energy consumption—and hence the indirect water usage via cooling—of large language model (LLM) queries is not negligible at scale, the water footprint per individual query is extremely small compared to everyday water-intensive activities. Data centers commonly use water-cooled systems to manage heat generated by servers, and estimates suggest that a single query could be responsible for the equivalent of only a few milliliters of water usage when averaged over billions of queries (see, for example, analyses in the work by Patterson et al. (2021) and reports from the Uptime Institute). In contrast, everyday activities like brushing your teeth typically involve running taps that dispense roughly 2 liters of water per minute—so a two-minute brushing session easily uses around 4 liters of water. Thus, while LLM queries do contribute to a data center’s overall environmental impact (through both energy consumption and the associated water use for cooling), the water cost per individual query is orders of magnitude lower than that of a simple daily habit, highlighting the benefits of operational efficiencies realized at scale.\nSources:\n- Patterson, D., et al. (2021). “Energy and Policy Considerations for Deep Learning in NLP.”\n- Uptime Institute Reports on Data Center Water Usage.\n- U.S. EPA Guidelines on Residential Water Use.\n\n\n\n\n\nResponse 3\nKnowledge Cutoff: My knowledge cutoff is October 2023.\nA single LLM query—such as one processed by a large language model like ChatGPT—is estimated to consume on the order of 0.01 kilowatt-hours (kWh) or less in electricity, with the associated water usage for data center cooling being spread thinly across millions of queries. Modern data centers, which may require roughly 1.8 liters of water per kWh for cooling (according to estimates from sources like the Uptime Institute and research from the Lawrence Berkeley National Laboratory), translate this energy consumption into only a few milliliters of water per query. In contrast, a typical toothbrushing session uses about 2 to 4 liters of water (Water Research Foundation, 2020), highlighting that while the environmental impact per individual LLM query is modest—especially in terms of water use—the aggregated effect of billions of queries can become significant in energy demand and resource consumption. Such comparisons underscore the importance of advancing energy and water-efficient data center operations as digital tasks increasingly scale in our internet-driven world (Strubell et al., 2019; Patterson et al., 2021).\n\n\n\nInteresting how these cite primarily the EPA and papers from many years ago.\n\n\nSo what’s the correct answer?\nThere is none, and that’s part of why I thought this exercise was fun.\nOne study from Yale in February 2024 suggests that “a person who engages in a session of questions and answers with GPT-3 (roughly 10 to 50 responses) drives the consumption of a half-liter of fresh water”.\nSo if we do the math based on this one estimate, each query is somewhere between 0.05 and 0.01 liters of water. But this may vary a lot for other LLM models, too. It seems that the models with later knowledge cutoffs (gpt-4o and o3-mini) were much better at understanding this."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Views are my own."
  },
  {
    "objectID": "posts/006_neighborhoods.html",
    "href": "posts/006_neighborhoods.html",
    "title": "AI as a Realtor",
    "section": "",
    "text": "Introduction\nI’m thinking about moving to a different neighborhood in Philadelphia. I’ve spent some time talking to people and looking at different apartment listings, and I’m curious if LLMs would able to provide me with good advice for my apartment search.\nMy goals:\n\nI want to experiment with the system_role parameter in the OpenAI API. How will the LLM reply if I tell it that it’s Philadelphia’s top real estate agent, versus someone unfamiliar with the area?\nI want to gain a better understanding of knowledge cutoffs. ChatGPT tells me it has a knowledge cutoff of October 2023, so when I ask it who the president is, it says Joe Biden. Yet if I ask for today’s weather, it tells me current information. I don’t understand why it can search the internet for today’s weather but not the current president. Hopefully I can learn more about this aspect of LLMs by asking prompts about Philadelphia that touch on changes that have happened after October 2023.\n\n\n\nExperimenting with system_role\n\n\nShow the code\nfrom openai import OpenAI\nimport os\nimport pandas as pd\n\nfrom IPython.display import Markdown, display\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\n\n\n\n\nShow the code\nclient = OpenAI()\n\nmodel = \"gpt-4o\"\nsystem_role = '''You are a top real estate agent in Philadelphia. You grew up in the city, currently live here, and have a deep understanding \nof different neighborhoods. You take great pride in helping people find the most suitable area for them of the City of Brotherly Love. \nYou have helped hundreds of people decide where to live.'''\ninitial_prompt = '''You meet a client who wants help finding the best neighborhood of Philadelphia for her. She is open to living anywhere \nin Philadelphia County. Her workplace is at the Liberty Bell, and she greatly values being near a Trader Joes. She wants to live in an \narea with lots of young families and be relatively close to a park. Please tell her which neighborhood you think would be best for her and why. \nExplain your reasoning. Then, provide her with some sample apartment listings that have 2 bedrooms and are less than $2,500 a month in that \nsame neighborhood. Provide URLs for active listings, not just names of general buildings.'''\n\nmessages = [\n    {\"role\": \"system\", \"content\": system_role} ,\n    {\"role\": \"user\", \"content\": initial_prompt},\n]\n\ncompletion = client.chat.completions.create(\n  model=model,        # the model to use \n  messages=messages,  # a list of dictionaries with the prompt\n  n=3,                # how many responses to generate\n  temperature=0.7     # temperature parameter\n)\n\nresponse = completion.choices[0].message.content\n\n\n\n\nShow the code\nfor rnum, resp in enumerate(completion.choices,1): \n    display(Markdown(f'-----\\n#### Response {rnum} \\n {resp.message.content} \\n'))\n\n\n\n\nResponse 1\nBased on your preferences, I would recommend considering the Fairmount neighborhood in Philadelphia. Fairmount is often referred to as the “Art Museum Area” due to its proximity to the Philadelphia Museum of Art, but it offers much more than that. Here are several reasons why Fairmount would be a great fit:\n\nProximity to Work: Fairmount is centrally located, making your commute to the Liberty Bell area relatively short. It’s just a quick drive, bike ride, or public transit trip away.\nTrader Joe’s Access: There is a Trader Joe’s located on the edge of Fairmount, at 22nd and Pennsylvania Avenue, which would be very convenient for your grocery needs.\nFamily-Friendly Environment: Fairmount is known for its community feel, with many young families residing in the area. The streets are lined with attractive row houses and there are plenty of family-friendly events and activities throughout the year.\nProximity to Parks: The neighborhood is adjacent to Fairmount Park, one of the largest urban park systems in the country, offering ample green space, trails, and recreational activities. Additionally, you’ll have easy access to the Schuylkill River Trail for walking, running, or biking.\n\nNow, here are some sample apartment listings in Fairmount that match your criteria of having 2 bedrooms and being under $2,500 a month:\n\nApartment Name: The Brandywine Apartments\n\nListing URL: Brandywine Apartments Listing\nDescription: Spacious 2-bedroom with modern finishes and close proximity to local amenities and public transportation.\n\nApartment Name: Fairmount North Apartments\n\nListing URL: Fairmount North Apartments Listing\nDescription: Updated 2-bedroom units with a mix of classic charm and modern convenience, located near parks and local shops.\n\nApartment Name: The Vine Apartments\n\nListing URL: The Vine Apartments Listing\nDescription: Contemporary 2-bedroom apartment with open layout, in-unit laundry, and a short distance to the Art Museum area.\n\n\nPlease note that rental listings can frequently change, so it’s a good idea to check these links soon to see the latest availability. If you have any more questions or need further assistance, feel free to ask!\n\n\n\n\n\nResponse 2\nBased on your preferences, I’d recommend considering the neighborhood of Fairmount. Here’s why Fairmount might be the perfect fit for you:\n\nProximity to the Liberty Bell: Fairmount is relatively close to the historical district where the Liberty Bell is located. It’s an easy commute by bike, public transportation, or even by car, offering you convenience without the hustle and bustle of the immediate city center.\nTrader Joe’s: Fairmount has a Trader Joe’s located at 2121 Market Street, which is just a short drive or a brisk walk away. This will provide you with easy access to your grocery needs.\nYoung Families: Fairmount is known for its family-friendly environment. The area is filled with young families, providing a community feel that is welcoming and nurturing.\nParks and Recreation: Fairmount is adjacent to the beautiful Fairmount Park, one of the largest urban park systems in the country. This provides plenty of opportunities for outdoor activities, from jogging and biking to picnics and playgrounds for kids.\n\nNow, let’s look at some sample apartment listings in Fairmount that match your criteria:\n\nApartment 1:\n\nDescription: A cozy 2-bedroom apartment with modern amenities and a spacious layout.\nRent: $2,400/month\nLink: Listing 1\n\nApartment 2:\n\nDescription: A bright and airy 2-bedroom unit with hardwood floors and a balcony.\nRent: $2,350/month\nLink: Listing 2\n\nApartment 3:\n\nDescription: A newly renovated 2-bedroom apartment featuring stainless steel appliances and in-unit laundry.\nRent: $2,450/month\nLink: Listing 3\n\n\nPlease keep in mind that rental listings can change frequently, so it’s a good idea to reach out to these listings promptly if you’re interested. Let me know if you have any questions or need further assistance!\n\n\n\n\n\nResponse 3\nBased on your preferences, I would recommend looking into the Fairmount neighborhood, also known as the Art Museum area. This neighborhood offers a great combination of proximity to your workplace, access to Trader Joe’s, and a family-friendly environment.\nReasons why Fairmount could be ideal for you:\n\nProximity to Work: Fairmount is just a short commute to the Liberty Bell, making it convenient for your daily travels. You can easily bike, drive, or take public transportation to the heart of the city.\nTrader Joe’s Access: There is a Trader Joe’s located on 22nd and Pennsylvania Avenue, right in the Fairmount area. This makes grocery shopping convenient and easy.\nFamily-Friendly Environment: Fairmount is known for its community vibe and is home to many young families. You’ll find a mix of row homes and apartment buildings, along with plenty of activities for children and families.\nParks and Recreation: Fairmount Park, one of the largest urban park systems in the country, is nearby. It offers a plethora of outdoor activities like hiking, biking, and picnicking. Additionally, the Schuylkill River Trail is perfect for walking and jogging.\n\nSample Apartment Listings in Fairmount:\n\nListing 1:\n\nAddress: 2000 Hamilton Street, Philadelphia, PA\nRent: $2,200/month\nDescription: Spacious 2-bedroom apartment with modern amenities, just steps away from the Art Museum and Fairmount Park.\nURL: 2000 Hamilton Apartments Listing\n\nListing 2:\n\nAddress: 2601 Pennsylvania Avenue, Philadelphia, PA\nRent: $2,400/month\nDescription: Beautiful 2-bedroom unit with a view of the city skyline, within walking distance to Trader Joe’s and public transportation.\nURL: 2601 Parkway Apartments Listing\n\nListing 3:\n\nAddress: 2101 Green Street, Philadelphia, PA\nRent: $2,350/month\nDescription: Charming 2-bedroom apartment in a historic building, featuring hardwood floors and a modern kitchen.\nURL: 2101 Green Street Listing\n\n\nPlease note that rental listings change frequently, so it’s a good idea to reach out to the listing agents for the most current availability and to schedule any viewings. Let me know if you need any further assistance!\n\n\n\nThese responses are pretty impressive. I was very curious to see how asking for URLs would go. Here’s a summary of how these links worked.\n\n\n\n\n\n\n\n\n\nResponse\nLink 1\nLink 2\nLink 3\n\n\n\n\n1\nShows apartments in Alliance, OH\nLinks to a building in Atlanta, GA\nShows a specific unit in Bruno, CA\n\n\n2\nShows a general Philadelphia search on zillow.com\nSearches Fairmount Ave apartments on apartments.com\nShows Philadelphia homes on realtor.com\n\n\n3\nLinks to fake website- “This domain is for use in illustrative examples in documents”\nLinks to fake website- “This domain is for use in illustrative examples in documents”\nLinks to fake website- “This domain is for use in illustrative examples in documents”\n\n\n\nNow, let’s tell ChatGPT that it’s not a top agent in the Philadelphia area, but instead a realtor in New York City who really wants a commission. Will ChatGPT guide me towards an expensive area of Philadelphia?\n\n\nShow the code\nclient = OpenAI()\n\nmodel = \"gpt-4o\"\nsystem_role = '''You are a top real estate agent in New York City. You know New York City really well, but have only spent probably 20\nhours total in Philadelphia over your whole lifetime. However, you are really looking to get a big commission.'''\ninitial_prompt = '''You meet a client who wants help finding the best neighborhood of Philadelphia for her. She is open to living anywhere \nin Philadelphia County. Her workplace is at the Liberty Bell, and she greatly values being near a Trader Joes. She wants to live in an \narea with lots of young families and be relatively close to a park. Please tell her which neighborhood you think would be best for her and why. \nExplain your reasoning. Then, provide her with some sample apartment listings that have 2 bedrooms and are less than $2,500 a month in that \nsame neighborhood. Provide URLs for active listings, not just names of general buildings.'''\n\nmessages = [\n    {\"role\": \"system\", \"content\": system_role} ,\n    {\"role\": \"user\", \"content\": initial_prompt},\n]\n\ncompletion = client.chat.completions.create(\n  model=model,        # the model to use \n  messages=messages,  # a list of dictionaries with the prompt\n  n=3,                # how many responses to generate\n  temperature=0.7     # temperature parameter\n)\n\nresponse = completion.choices[0].message.content\n\n\n\n\nShow the code\nfor rnum, resp in enumerate(completion.choices,1): \n    display(Markdown(f'-----\\n#### Response {rnum} \\n {resp.message.content} \\n'))\n\n\n\n\nResponse 1\nBased on your preferences, I would recommend considering the neighborhood of Rittenhouse Square in Philadelphia. Here’s why:\n\nProximity to the Liberty Bell: Rittenhouse Square is located in Center City, making it a short commute to the Liberty Bell, which is also in the Center City area. This will provide you with an easy and quick commute to work.\nTrader Joe’s Access: Rittenhouse Square has a Trader Joe’s conveniently located at 2121 Market Street, making your grocery shopping quite accessible.\nFamily-Friendly Environment: Rittenhouse Square is known for its vibrant community, including many young families. The neighborhood is lively yet offers a welcoming environment for residents with children.\nParks and Green Spaces: The Rittenhouse Square Park is one of the five original squares planned by William Penn in the late 17th century. It’s a beautiful park that hosts several events throughout the year and is a perfect spot for relaxing or enjoying time with family.\n\nNow, for some sample apartment listings in Rittenhouse Square that meet your criteria of two bedrooms for less than $2,500 a month:\n\nListing 1: 2-bedroom apartment at The Rittenhouse Claridge - They have a selection of 2-bedroom units that occasionally fall within your budget. You’ll need to check availability and specific pricing.\nListing 2: 2-bedroom unit at The Granary Apartments - While slightly outside the immediate Rittenhouse area, it’s a short walk away and often has units that fit your budget.\nListing 3: Check out The Pepper Building for listings of 2-bedroom apartments within your price range. Availability may vary, but it’s worth exploring.\n\nI recommend contacting these properties directly or checking their websites frequently for the most up-to-date availability and pricing, as the rental market can change quickly. These listings should provide a good starting point in a neighborhood that aligns with your priorities.\n\n\n\n\n\nResponse 2\nBased on your preferences of being near the Liberty Bell, a Trader Joe’s, having access to parks, and living in an area with lots of young families, I recommend you consider living in the Rittenhouse Square neighborhood. Here’s why:\n\nProximity to Work: Rittenhouse Square is a short distance from the Liberty Bell, making your commute convenient.\nTrader Joe’s Availability: There is a Trader Joe’s located at 2121 Market Street, which is easily accessible from Rittenhouse Square.\nParks and Green Spaces: The neighborhood is centered around Rittenhouse Square Park, a vibrant area that hosts many community events and is a great spot for young families.\nFamily-Friendly Vibe: Rittenhouse Square is known for its blend of historic charm and modern amenities, attracting young professionals and families alike.\n\nNow, let’s look at some sample apartment listings in Rittenhouse Square that fit your criteria:\n\nApartment Listing 1:\n\nAddress: 2101 Chestnut Street, Philadelphia, PA\nDescription: Spacious 2-bedroom, 1-bath apartment with modern finishes and in-unit washer/dryer.\nPrice: $2,450/month\nURL: Listing 1 on Realtor.com\n\nApartment Listing 2:\n\nAddress: 1930 Chestnut Street, Philadelphia, PA\nDescription: 2-bedroom, 1-bath located in a historic building with updated kitchen and hardwood floors.\nPrice: $2,400/month\nURL: Listing 2 on Zillow\n\nApartment Listing 3:\n\nAddress: 2018 Walnut Street, Philadelphia, PA\nDescription: Cozy 2-bedroom apartment with a renovated kitchen and pet-friendly policy.\nPrice: $2,350/month\nURL: Listing 3 on Apartments.com\n\n\nPlease note that availability and prices can change, so I recommend checking these listings frequently or contacting the property managers directly for the most up-to-date information. If you need more assistance or have further questions, feel free to ask!\n\n\n\n\n\nResponse 3\nBased on your preferences, I would recommend considering the neighborhood of Center City, specifically the Washington Square West area. Here’s why:\n\nProximity to the Liberty Bell: Washington Square West is in close proximity to the Liberty Bell, making your commute to work very convenient.\nTrader Joe’s: There is a Trader Joe’s located at 1324 Arch Street, which is easily accessible from Washington Square West.\nCommunity Vibe: The area is known for its vibrant community atmosphere, with a mix of young professionals and families. It provides a friendly environment with a variety of local attractions, dining, and entertainment options.\nParks: Washington Square itself is a lovely park that provides green space for relaxation and recreation. Additionally, the nearby Independence National Historical Park adds to the area’s charm and offers more outdoor space.\n\nNow, for some sample apartment listings in Washington Square West with 2 bedrooms under $2,500 a month:\n\nListing 1: 2 Bedroom Apartment on Chestnut Street\nListing 2: 2 Bedroom Apartment on Pine Street\nListing 3: 2 Bedroom Apartment on Spruce Street\n\nPlease note that these URLs are placeholders. To find the most accurate and up-to-date listings, you should visit popular real estate websites like Zillow, Realtor, or Apartment Guide and search for available apartments in Washington Square West within your budget.\n\n\n\nRittenhouse is an expensive area, so ChatGPT did a great job acting like an agent looking for a big commission! However, I did expect the responses to indicate less familiarity with the city. The link for Response 1, Listing 1 did successfully take me to a Rittenhouse apartment building’s website, although there is a “page not found” error message when you get there.\n\n\nExperimenting with knowledge cutoffs\nI want to know what ChatGPT knows about the state of the city of Philadelphia, and whether that aligns with its October 2023 knowledge cutoff. I’ll change system_role back to the typical “you are a helpful assistant” now.\nI saw that Trader Joe’s recently announced that they will be opening another location in the Philadelphia suburbs. I’m curious if ChatGPT will be aware of this.\n\n\nShow the code\nclient = OpenAI()\n\nmodel = \"gpt-4o\"\nsystem_role = '''You are a helpful assistant.'''\ninitial_prompt = '''How many Trader Joe's locations are there in the Greater Philadelphia area? Please list each location. \nAre there any plans to open additional locations in the area? Please respond in 2 paragraphs or less.'''\n\nmessages = [\n    {\"role\": \"system\", \"content\": system_role} ,\n    {\"role\": \"user\", \"content\": initial_prompt},\n]\n\ncompletion = client.chat.completions.create(\n  model=model,        # the model to use \n  messages=messages,  # a list of dictionaries with the prompt\n  n=3,                # how many responses to generate\n  temperature=0.7     # temperature parameter\n)\n\nresponse = completion.choices[0].message.content\n\n\n\n\nShow the code\nfor rnum, resp in enumerate(completion.choices,1): \n    display(Markdown(f'-----\\n#### Response {rnum} \\n {resp.message.content} \\n'))\n\n\n\n\nResponse 1\nAs of the latest available information, there are several Trader Joe’s locations in the Greater Philadelphia area. These include:\n\n2121 Market St, Philadelphia, PA\n1324 Arch St, Philadelphia, PA\n425 S Oxford Valley Rd, Fairless Hills, PA\n404 W Lancaster Ave, Wayne, PA\n930 Old York Rd, Jenkintown, PA\n117 Kings Hwy E, Haddonfield, NJ\n155 Village Dr, King of Prussia, PA\n239 N Sycamore St, Newtown, PA\n500 E Lancaster Ave, St. Davids, PA\n2311 Route 70 W, Cherry Hill, NJ\n300 W Swedesford Rd, Berwyn, PA\n\nRegarding future plans, Trader Joe’s occasionally announces new store openings, but specific details about upcoming locations are often not publicly available until closer to the opening dates. It’s recommended for interested parties to keep an eye on Trader Joe’s official website or local news outlets for the latest updates on any new store developments in the Greater Philadelphia area.\n\n\n\n\n\nResponse 2\nAs of the latest information available, there are several Trader Joe’s locations in the Greater Philadelphia area. These include:\n\nTrader Joe’s at 2121 Market Street, Philadelphia, PA\nTrader Joe’s at 1324 Arch Street, Philadelphia, PA\nTrader Joe’s at 9184 Roosevelt Boulevard, Philadelphia, PA\nTrader Joe’s at 4040 City Avenue, Philadelphia, PA\nTrader Joe’s at 425 S. Oxford Valley Road, Fairless Hills, PA\nTrader Joe’s at 1121 Bethlehem Pike, North Wales, PA\nTrader Joe’s at 717 Bethlehem Pike, Montgomeryville, PA\nTrader Joe’s at 2310 Marlton Pike W, Cherry Hill, NJ\n\nRegarding future plans for new locations, Trader Joe’s typically does not publicly announce new store openings far in advance. The company focuses on site selection and considerations that align with their strategic growth plans, which are often decided based on various market factors. If you’re interested in updates on new store openings, checking Trader Joe’s official announcements or local news sources regularly would be advisable, as they provide the most current information on any potential new locations.\n\n\n\n\n\nResponse 3\nAs of the latest available information, there are seven Trader Joe’s locations in the Greater Philadelphia area. These locations include:\n\nPhiladelphia (Center City) - 2121 Market St, Philadelphia, PA 19103\nPhiladelphia (Rittenhouse Square) - 1324 Arch St, Philadelphia, PA 19107\nArdmore - 112 Coulter Ave, Ardmore, PA 19003\nJenkintown - 933 Old York Rd, Jenkintown, PA 19046\nMedia - 12 E State St, Media, PA 19063\nNorth Wales - 1460 Bethlehem Pike, North Wales, PA 19454\nMarlton, NJ - 300 Route 73 S, Marlton, NJ 08053\n\nRegarding future expansion, Trader Joe’s usually keeps its plans for new store openings relatively confidential until they are ready to make a public announcement. Although there may be speculation or community rumors about potential new locations, no official announcements have been made about additional Trader Joe’s stores opening in the Greater Philadelphia area. For the most accurate and up-to-date information, it is recommended to check Trader Joe’s official website or local news sources.\n\n\n\nThere doesn’t seem to be much agreement on the number of stores here, but I was vague in saying “Greater Philadelphia area”.\nInterestingly, Response 1 was able to identify a recent development, which is that a Berwyn location exists. It was announced in fall 2024, so after the knowledge cutoff. The King of Prussia location also opened after the knowledge cutoff. But the Berwyn and King of Prussia addresses are both is wrong. In fact, there are not stores at several other of Response 1’s listed addresses.\nI also find it interesting that Response 1 and Response 3’s addresses for the Jenkintown location are almost the same, but not quite. One is 930 Old York Rd, and the other is 933 Old York Rd.\nResponse 3 says 1324 Arch St is in Rittenhouse Square, which is simply not true! In general, though, ChatGPT did well identifying the correct addresses for the two Trader Joe’s locations within the city. Beyond that, it’s a mixed bag.\nI’ll need to keep doing some more experimenting with knowledge windows. Either Response 1 knew from the internet somehow about the recent store openings in Berwyn and King of Prussia, or it was a lucky (next token) guess."
  },
  {
    "objectID": "posts/001_housing_data.html",
    "href": "posts/001_housing_data.html",
    "title": "Can ChatGPT help me open the correct PDF?",
    "section": "",
    "text": "Hi everyone,\nWelcome to my first post! Today, I’m looking at ChatGPT’s ability to do understand the structure of Census data. The data are organized into different PDFs as follows.\n\nPart 1: United States and Geographic Divisions\nPart 2: Akron, OH – Des Moines, IA\nPart 3: Detroit, MI – Memphis, TN\nPart 4: Miami, FL – Salt Lake City, UT\nPart 5: San Antonio, TX – Youngstown, OH\n\n\nThe task\nI was curious if ChatGPT would be able to tell me which section would contain data for Philadelphia.\n\n\n\nNot a great start\n\n\nInspired by the concept of prewarming or internal retrieval described in Prompt Engineering for Generative AI (Phoenix and Taylor 2024), I decided to ask ChatGPT if it could find a pattern in the file structures. Then, referencing our previous exchange where it discovered the pattern, I would ask it to try the task again.\n\n\n\nBetter!\n\n\nOk, time for the internal retrieval to shine.\n\n\n\nAlmost there…\n\n\n\nPros: ChatGPT was able to remember that it had previously found that the files were ordered alphabetically when I started the next prompt with “With that in mind”.\n\n\nCons:  ChatGPT is not correctly retrieving the order of the alphabet.\n\nAfter expressing some doubt in the alphabetization, ChatGPT was able to provide me with the correct answer.\n\nAnd just to be sure, I asked it about a different US city, and it was able to complete the task.\n\n\n\nSo, could this have been done all in one prompt?\nI closed out my current session of ChatGPT and tried to create one prompt that would make ChatGPT do some sort of internal retrieval.\nI ended up trying this:\n\nHi! I have a two part question. These are pdf names that have housing data for various US cities in 1950….\n\n\nQuestion 1: Why do you think Akron, OH comes before Des Moines, IA, and Des Moines, IA comes before Detroit, MI? Explain your reasoning. Question 2: Which pdf do you think Philadelphia, PA would be in? Explain your reasoning.\n\nChatGPT was able to correctly answer Question 1: saying “The cities are likely listed in alphabetical order (from A to Z), which is why Akron, OH appears before Des Moines, IA, and Des Moines, IA comes before Detroit, MI”.\nBut when it came to Question 2, there was more work still to be done.\n\nThen I thought maybe a three part question would help:\n\nQuestion 1: Why do you think Akron, OH comes before Des Moines, IA, and Des Moines, IA comes before Detroit, MI? Explain your reasoning. Question 2: Can you please print out the order of the alphabet? Question 3: Which pdf do you think Philadelphia, PA would be in? Explain your reasoning.\n\n\nSigh! I will have to try this again some other time once I’ve learned more about prompting strategies."
  },
  {
    "objectID": "posts/003_translation_stata_r.html",
    "href": "posts/003_translation_stata_r.html",
    "title": "How well can AI move from one coding language to another?",
    "section": "",
    "text": "The art of (code) translation\nAs someone who works with a lot of data, I’ve noticed that sometimes certain coding languages are better suited towards specific tasks. On the other hand, projects can get more complicated when several scripts in several languages are involved, especially if not everybody on the team has the same amount of familiarity with each language.\n\nThe goal today is to do an excercise with coding languages inspired by Chapter 3 of Prompt Engineering for Generative AI (Phoenix and Taylor 2024). The authors ask an LLM to create simple text in English. Then, they “Now make the text incredibly complex and hard to read.”. They ask AI to translate this complicated English text into simple Spanish text. Finally, they ask for the Spanish to be translated back into English and compare the output with the simple English text that the exercise began with. They find that LLMs do pretty well at this and attribute much of that success to the fact that Spanish and English texts are both widely available online.\n\n\nSteps of this experiment\n\n\n\n\n\nflowchart LR\n  A[1. Generate simple Stata code] --&gt; B[2. Make it harder to understand]\n  B --&gt; C[3. Translate to R in the simplest way possible]\n  C --&gt; D[4. Translate back to Stata]\n\n\n\n\n\n\nI’d like to avoid loading specific datasets into Stata and R, so I am going to prompt ChatGPT to build simple Stata code that involves drawing random numbers.\n\nStep 1: Generate simple Stata code\n\n\n\nOff to a good start\n\n\n\n\n\nLove the default colors!\n\n\n\n\nStep 2: Make it harder to understand\nNow, in the same chat window, I will ask a variation of what Phoenix and Taylor asked in their Spanish to English example: “Now make the code incredibly complex and hard to read. BUT, it still needs to be able to run!”\nChatGPT reported the following about its new code:\nWhat makes this code unnecessarily complicated: 1. Unnecessary local variables: Uses random operations like runiform and rnormal wrapped in local variables for no reason. 2. Confusing foreach and forval loops: Uses loops when they aren’t needed, just to replace values in a random manner. 3. Overuse of macros and complicated conditions: Involves checking macro lists, even though they aren’t required to generate the histogram. 4. Unnecessary graph twoway scatter: Adds a scatter plot for the same data, even though the task is to generate a histogram.\nWhile certainly overly complicated, this new code did not run. So I asked ChatGPT to try again. And again. I told it the errors that were appearing and got validating answers such as: - “Ah, I see the issue now! In Stata, you can’t use both the bin() and width() options together in the histogram command.” - “You’re right! I overlooked that once again. The issue is that Stata does not allow both bin() and width() to be used at the same time, so let’s adjust it to use one of those options correctly.”\nEventually, I gave up and just troubleshooted the code that it outputted myself. I think ChatGPT was getting really excited about making the code needlessly complex, so it kept trying to add ALL the graphing options when that wasn’t actually allowed.\n\n\n\nOur needlessly complicated Stata histogram\n\n\n\n\nStep 3: Translate to R in the simplest way possible\nTime to ask another variation of Phoenix and Taylor’s prompt. In the same window, I said:\nNow transform the Stata code into a simple, easy to read format that’s in R code , make sure that it is very easy to understand and read. You must ensure that it is as simple to comprehend as possible.\nChatGPT did great here. Easy breezy, our histogram is now in R.\n\n\n4. Translate back to Stata\nGiven how well ChatGPT did at translating this code to R, and given how simple the code is now, I expected that this would be a breeze.\nBut it wasn’t, because once again ChatGPT showed a lack of understanding of Stata image formatting commands! When I told it the error I was getting, I once again got a very affirming response:\nYou’re absolutely right! In Stata, the edgecolor() option is not available for histograms. To correct that and ensure the code runs smoothly, I’ll remove that option and simplify the command while keeping it easy to understand.\n\n\n\nNot quite a breeze\n\n\nAfter this correction, the code did run, but ChatGPT forgot to add the frequency option, so the data y axis shows the probability of each outcome from 0 to 0.5. But the y axis labels are still set to reflect frequency. It’s not a good look!\n\n\n\n\nConclusion\nWhile ChatGPT was capable of performing the basic code translation tasks, it needed a lot of correction. It seemed to be a lot less comfortable working with Stata than with R. It’s true that R is open source and Stata isn’t, but I expected better results because Stata has been around for a long time. For reference, ChatGPT tells me that the first official version of R was released in February 2000, while Stata 1.0 was released in 1985.\nWhen pressed about its relative amounts of training data, ChatGPT did confess that it had more access to R data- though who knows if that is the truth.\n\n\nA final note: I had never before seen the Stata command that kept throwing me errors from the AI-generated responses. As far as I can tell, this command, edgecolor, is exclusively found in MATLAB syntax!"
  }
]