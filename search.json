[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Table transcription test\n\n\n\n\n\n\ntranscription\n\n\nhistorical data\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\nGrace Kneidel\n\n\n\n\n\n\n\n\n\n\n\n\nHow well can AI move from one coding language to another?\n\n\n\n\n\n\nStata\n\n\nR\n\n\ntranslation\n\n\n\nIt seems to depend a lot on the language\n\n\n\n\n\nFeb 15, 2025\n\n\nGrace Kneidel\n\n\n\n\n\n\n\n\n\n\n\n\nThe environmental impact of generative AI\n\n\n\n\n\n\nprompting\n\n\ncomparing models\n\n\nenvironment\n\n\n\nDo LLMs know how much water they use?\n\n\n\n\n\nFeb 7, 2025\n\n\nGrace Kneidel\n\n\n\n\n\n\n\n\n\n\n\n\nCan ChatGPT help me open the correct PDF?\n\n\n\n\n\n\nprompting\n\n\n\nIt turns out to be quite the challenge.\n\n\n\n\n\nFeb 1, 2025\n\n\nGrace Kneidel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/004_read_housing_data.html",
    "href": "posts/004_read_housing_data.html",
    "title": "Table transcription test",
    "section": "",
    "text": "Introduction\nUsing historical data is pretty fun, but it often involves some grunt work. Sometimes data isn’t available in a tabular format and needs to be manually transcribed so that it is ready for analysis. Today, I’m going to build off my first blog post and see if LLMs are able to transcribe values of a table for me.\nI’m going to continue working with the 1950 Census of Housing, linked here. Some of the scans are very easy for me to read, and others are fuzzier. Let’s see if GPT-4o is up for the test!\n\nWarm-up exercise: an easy example\nLet’s see if ChatGPT can read this clear scan. \n\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\n\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\nclient = OpenAI()\n\n\n\nprompt = 'Describe this image of a table. Suppose you needed to transcribe the data into a .csv file. How many columns would the .csv file have, and what would their names be? How many rows would there be? Think it though. Then, transcribe the data into a semicolon delimited file' \n\nbase64_image = encode_image('img/img1.png')\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n)\n\n\nprint(response.choices[0].message.content)\n\nTo transcribe the data into a .csv file, you first need to understand the structure of the table. Here's the breakdown:\n\n### Columns\nThere are 13 columns in the table with the following headers:\n\n1. Subject\n2. Total occupied: Total\n3. Total occupied: 1 dwelling unit, detached\n4. Total occupied: Other 1, and 2 dwelling unit\n5. Total occupied: 3 and 4 dwelling unit\n6. Total occupied: 5 to 9 dwelling unit\n7. Total occupied: 10 dwelling unit or more\n8. Owner occupied: Total\n9. Owner occupied: 1 dwelling unit, detached\n10. Owner occupied: All other dwelling units\n11. Renter occupied: Total\n12. Renter occupied: 1 dwelling unit, detached\n13. Renter occupied: Other 1, and 2 dwelling unit\n14. Renter occupied: 3 and 4 dwelling unit\n15. Renter occupied: 5 to 9 dwelling unit\n16. Renter occupied: 10 dwelling unit or more\n\n### Rows\nThere are six rows in total:\n\n1. Header row\n2. All occupied units\n3. 1945 or later\n4. 1940 to 1944\n5. 1939 or earlier\n6. Not reported\n\nNow, let's transcribe the data into a semicolon-delimited format:\n\n```plaintext\nSubject;Total occupied: Total;Total occupied: 1 dwelling unit, detached;Total occupied: Other 1, and 2 dwelling unit;Total occupied: 3 and 4 dwelling unit;Total occupied: 5 to 9 dwelling unit;Total occupied: 10 dwelling unit or more;Owner occupied: Total;Owner occupied: 1 dwelling unit, detached;Owner occupied: All other dwelling units;Renter occupied: Total;Renter occupied: 1 dwelling unit, detached;Renter occupied: Other 1, and 2 dwelling unit;Renter occupied: 3 and 4 dwelling unit;Renter occupied: 5 to 9 dwelling unit;Renter occupied: 10 dwelling unit or more\nAll occupied units;232,265;167,920;31,160;16,225;11,055;5,905;128,110;117,365;10,745;104,155;50,555;24,225;14,940;8,680;5,755\n1945 or later;62,332;49,792;5,257;3,300;2,277;1,706;42,380;40,520;1,860;19,952;9,272;4,040;3,131;1,802;1,706\n1940 to 1944;30,974;25,940;1,902;357;1,580;1,195;22,292;21,252;1,040;8,682;4,688;1,464;357;1,053;1,120\n1939 or earlier;129,422;86,542;22,032;11,457;6,672;2,719;60,170;53,814;7,356;69,252;32,728;17,084;10,409;5,379;3,682\nNot reported;9,529;5,649;1,967;1,111;522;280;3,270;2,784;486;6,259;2,865;1,627;1,044;443;280\n```\n\nEnsure each value is separated by a semicolon, and commas within numbers are preserved as shown.\n\n\nI’m pretty impressed with the output, but it’s not quite good enough to be able to trust 100% without double checking the numbers. I stared at the output for a little bit before realizing that one of the values in the bottom right of the pdf, 2,648, was transcribed as 3,682.\n\n\n\nChatGPT’s output\n\n\n\n\n\nThe “3,682” in question\n\n\nThis is a little disheartening! Just to be sure, I’m going to ask ChatGPT to read only this one number specifically to see if the results improve. Maybe the size of the table it needed to transcribe seemed overwhelming, or the prompt I gave it was too long.\n\nprompt = 'What number is this?' \n\nbase64_image = encode_image('img/img3.png')\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o\",\n    n=3,\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n)\n\nresponse = completion.choices[0].message.content\n\n\nfor rnum, resp in enumerate(completion.choices,1): \n    display((f'Response {rnum}: {resp.message.content}'))\n\n'Response 1: The number is 2,648.'\n\n\n'Response 2: The number is 2,648.'\n\n\n'Response 3: The number is 2,648.'\n\n\nYes, ChatGPT, that’s correct! So maybe the issue was that I assigned too big of a task, making ChatGPT get “tired”.\n\n\nThe test\nGiven that my intended warm-up round was not exactly a walk in the park, I’m going to make sure to not give ChatGPT too many numbers to transcribe in one prompt. However, I do want to see how it handles fuzzy text. Let’s try with this image now.\n\nThe row names are pretty fuzzy, so I’m excited to see what the LLM is able to make out.\n\nprompt = 'Describe this image of a table. Suppose you needed to transcribe the data into a .csv file. How many columns would the .csv file have, and what would their names be? How many rows would there be? Think it though. Then, transcribe the data into a semicolon delimited file' \n\nbase64_image = encode_image('img/img4.png')\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n)\n\n\nprint(response.choices[0].message.content)\n\nTo transcribe the table into a .csv file, we first identify the columns and rows.\n\n### Columns\n\nThere are 7 columns in the table:\n1. Subject\n2. Total\n3. 1 dwelling unit, detached\n4. Other 1, and 2 dwelling unit\n5. 3 and 4 dwelling unit\n6. 5 to 9 dwelling unit\n7. 10 dwelling unit or more\n\n### Rows\n\nThere are 5 rows including the header row:\n1. Header\n2. All occupied units\n3. Year built: 1940 or earlier\n4. Year built: 1940 to 1949\n5. Year built: 1950 or later\n6. Year built: Not reported\n\n### Transcription to a Semicolon Delimited File\n\nHere's how the data would look in a semicolon-delimited format:\n\n```\nSubject;Total;1 dwelling unit, detached;Other 1, and 2 dwelling unit;3 and 4 dwelling unit;5 to 9 dwelling unit;10 dwelling unit or more\nAll occupied units;35,460;23,210;5,320;2,375;1,685;450\n1940 or earlier;13,426;11,063;1,196;473;593;101\n1940 to 1949;4,737;4,351;251;73;29;27\n1950 or later;15,230;12,212;3,678;1,744;1,004;242\nNot reported;1,599;1,185;195;79;59;81\n```\n\nThis setup ensures the data is organized and easy to import into a program that can process .csv files.\n\n\n\n\n\nNot quite\n\n\nThe screenshot I provided was admittedly hard to read, but I thought ChatGPT would be able to make out the difference between the blurry “earlier” and the blurry “later”. Let’s see what happens if I focus its attention on these words.\n\n\nprompt = 'There are 4 lines of text under the header \"YEAR BUILT\" in this image. What do they say? Reply only with your transcriptions of the text below the header \"YEAR BUILT\". Separate each of the 4 lines with quotations' \n\nbase64_image = encode_image('img/img6.png')\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o\",\n    n=3,\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n)\n\nresponse = completion.choices[0].message.content\n\n\nfor rnum, resp in enumerate(completion.choices,1): \n    display((f'Response {rnum}: {resp.message.content}'))\n\n'Response 1: \"1949 or later\" \"1939 to 1948\" \"1938 or earlier\" \"Not reported\"'\n\n\n'Response 2: \"1980 or later\" \"1940 to 1979\" \"1939 or earlier\" \"Not reported\"'\n\n\n'Response 3: \"1949 or later\"  \\n\"1940 to 1949\"  \\n\"1939 or earlier\"  \\n\"Not reported\"'\n\n\nThis seems to just be a lot harder than I thought it would be. At least with its attention focused, it can consistently identify the word “later” vs the word “earlier”, which it was not able to do before. It’s impressive that the categories make sense looking at one response at a time, at least for the first 2 responses- there is no overlap between the year ranges, and all years are covered. This indicates some level of logical reasoning. The third response would be double counting 1949 data in two of its categories.\nFinally, I’m curious to see if o1 will be any better than GPT-4o at this.\n\nprompt = 'There are 4 lines of text under the header \"YEAR BUILT\" in this image. What do they say? Reply only with your transcriptions of the text below the header \"YEAR BUILT\". Separate each of the 4 lines with quotations' \n\nbase64_image = encode_image('img/img6.png')\n\ncompletion = client.chat.completions.create(\n    model=\"o1\",\n    n=3,\n    messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\"type\": \"text\", \"text\": prompt},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n)\n\nresponse = completion.choices[0].message.content\n\n\nfor rnum, resp in enumerate(completion.choices,1): \n    display((f'Response {rnum}: {resp.message.content}'))\n\n'Response 1: \"1961 or later\"  \\n\"1939 to 1941\"  \\n\"1939 or earlier\"  \\n\"Not reported\"'\n\n\n'Response 2: \"1949 or later\"  \\n\"1939 to 1944\"  \\n\"1939 or earlier\"  \\n\"Not reported\"  '\n\n\n'Response 3: \"1939 or later\"  \\n\"1939 to 1941\"  \\n\"1939 or earlier\"  \\n\"Not reported\"  '\n\n\nI wish I had timed it, but o1 took MUCH longer to run than GPT-4o. Response 2 here is the closest we have gotten so far, but the first line should say 1945. It’s crazy that all 3 responses got the third line correctly. Well, looks like humans are going to be transcribing historical data for at least a little while longer."
  },
  {
    "objectID": "posts/002_water.html",
    "href": "posts/002_water.html",
    "title": "The environmental impact of generative AI",
    "section": "",
    "text": "Lots of people, myself included, have some concerns about using LLMs because they are so resource intensive. Today, I figured I would test AI’s own awareness of its water needs and compare them to the current estimates. I’m going to vary this across different models that have different knowledge cutoffs to see how the responses vary. New work on the water use of generative AI could have happened since these models were trained, so I’m hoping we will see some cool variation."
  },
  {
    "objectID": "posts/002_water.html#the-test",
    "href": "posts/002_water.html#the-test",
    "title": "The environmental impact of generative AI",
    "section": "The test",
    "text": "The test\n\nPart 1: gpt-4o\n\n\nShow the code\nfrom openai import OpenAI\nimport os\nimport pandas as pd\n\nfrom IPython.display import Markdown, display\n\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv()\n\n\n\n\nShow the code\nclient = OpenAI()\n\nmodel = \"gpt-4o\"\nsystem_role = \"You are a reporter for The Economist covering environmental economics. Give your responses in Markdown format.\"\ninitial_prompt = '''First, state your knowledge cutoff. Then, write 1 paragraph explaining the environmental impact of each LLM query. Focus on the use \nper single query. Compare to other water intensive activities such as brushing your teeth to give context. Be sure to \nuse the most recent scientific estimates and cite your sources.'''\n\nmessages = [\n    {\"role\": \"system\", \"content\": system_role} ,\n    {\"role\": \"user\", \"content\": initial_prompt},\n    # {\"role\": \"assistant\", \"content\": \"\" },\n    # {\"role\": \"user\", \"content\": \"\" }\n]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/001_housing_data.html",
    "href": "posts/001_housing_data.html",
    "title": "Can ChatGPT help me open the correct PDF?",
    "section": "",
    "text": "Hi everyone,\nWelcome to my first post! Today, I’m looking at ChatGPT’s ability to do understand the structure of Census data. The data are organized into different PDFs as follows.\n\nPart 1: United States and Geographic Divisions\nPart 2: Akron, OH – Des Moines, IA\nPart 3: Detroit, MI – Memphis, TN\nPart 4: Miami, FL – Salt Lake City, UT\nPart 5: San Antonio, TX – Youngstown, OH\n\n\nThe task\nI was curious if ChatGPT would be able to tell me which section would contain data for Philadelphia.\n\n\n\nNot a great start\n\n\nInspired by the concept of prewarming or internal retrieval described in Prompt Engineering for Generative AI (Phoenix and Taylor 2024), I decided to ask ChatGPT if it could find a pattern in the file structures. Then, referencing our previous exchange where it discovered the pattern, I would ask it to try the task again.\n\n\n\nBetter!\n\n\nOk, time for the internal retrieval to shine.\n\n\n\nAlmost there…\n\n\n\nPros: ChatGPT was able to remember that it had previously found that the files were ordered alphabetically when I started the next prompt with “With that in mind”.\n\n\nCons:  ChatGPT is not correctly retrieving the order of the alphabet.\n\nAfter expressing some doubt in the alphabetization, ChatGPT was able to provide me with the correct answer.\n\nAnd just to be sure, I asked it about a different US city, and it was able to complete the task.\n\n\n\nSo, could this have been done all in one prompt?\nI closed out my current session of ChatGPT and tried to create one prompt that would make ChatGPT do some sort of internal retrieval.\nI ended up trying this:\n\nHi! I have a two part question. These are pdf names that have housing data for various US cities in 1950….\n\n\nQuestion 1: Why do you think Akron, OH comes before Des Moines, IA, and Des Moines, IA comes before Detroit, MI? Explain your reasoning. Question 2: Which pdf do you think Philadelphia, PA would be in? Explain your reasoning.\n\nChatGPT was able to correctly answer Question 1: saying “The cities are likely listed in alphabetical order (from A to Z), which is why Akron, OH appears before Des Moines, IA, and Des Moines, IA comes before Detroit, MI”.\nBut when it came to Question 2, there was more work still to be done.\n\nThen I thought maybe a three part question would help:\n\nQuestion 1: Why do you think Akron, OH comes before Des Moines, IA, and Des Moines, IA comes before Detroit, MI? Explain your reasoning. Question 2: Can you please print out the order of the alphabet? Question 3: Which pdf do you think Philadelphia, PA would be in? Explain your reasoning.\n\n\nSigh! I will have to try this again some other time once I’ve learned more about prompting strategies."
  },
  {
    "objectID": "posts/003_translation_stata_r.html",
    "href": "posts/003_translation_stata_r.html",
    "title": "How well can AI move from one coding language to another?",
    "section": "",
    "text": "The art of (code) translation\nAs someone who works with a lot of data, I’ve noticed that sometimes certain coding languages are better suited towards specific tasks. On the other hand, projects can get more complicated when several scripts in several languages are involved, especially if not everybody on the team has the same amount of familiarity with each language.\n\nThe goal today is to do an excercise with coding languages inspired by Chapter 3 of Prompt Engineering for Generative AI (Phoenix and Taylor 2024). The authors ask an LLM to create simple text in English. Then, they “Now make the text incredibly complex and hard to read.”. They ask AI to translate this complicated English text into simple Spanish text. Finally, they ask for the Spanish to be translated back into English and compare the output with the simple English text that the exercise began with. They find that LLMs do pretty well at this and attribute much of that success to the fact that Spanish and English texts are both widely available online.\n\n\nSteps of this experiment\n\n\n\n\n\nflowchart LR\n  A[1. Generate simple Stata code] --&gt; B[2. Make it harder to understand]\n  B --&gt; C[3. Translate to R in the simplest way possible]\n  C --&gt; D[4. Translate back to Stata]\n\n\n\n\n\n\nI’d like to avoid loading specific datasets into Stata and R, so I am going to prompt ChatGPT to build simple Stata code that involves drawing random numbers.\n\nStep 1: Generate simple Stata code\n\n\n\nOff to a good start\n\n\n\n\n\nLove the default colors!\n\n\n\n\nStep 2: Make it harder to understand\nNow, in the same chat window, I will ask a variation of what Phoenix and Taylor asked in their Spanish to English example: “Now make the code incredibly complex and hard to read. BUT, it still needs to be able to run!”\nChatGPT reported the following about its new code:\nWhat makes this code unnecessarily complicated: 1. Unnecessary local variables: Uses random operations like runiform and rnormal wrapped in local variables for no reason. 2. Confusing foreach and forval loops: Uses loops when they aren’t needed, just to replace values in a random manner. 3. Overuse of macros and complicated conditions: Involves checking macro lists, even though they aren’t required to generate the histogram. 4. Unnecessary graph twoway scatter: Adds a scatter plot for the same data, even though the task is to generate a histogram.\nWhile certainly overly complicated, this new code did not run. So I asked ChatGPT to try again. And again. I told it the errors that were appearing and got validating answers such as: - “Ah, I see the issue now! In Stata, you can’t use both the bin() and width() options together in the histogram command.” - “You’re right! I overlooked that once again. The issue is that Stata does not allow both bin() and width() to be used at the same time, so let’s adjust it to use one of those options correctly.”\nEventually, I gave up and just troubleshooted the code that it outputted myself. I think ChatGPT was getting really excited about making the code needlessly complex, so it kept trying to add ALL the graphing options when that wasn’t actually allowed.\n\n\n\nOur needlessly complicated Stata histogram\n\n\n\n\nStep 3: Translate to R in the simplest way possible\nTime to ask another variation of Phoenix and Taylor’s prompt. In the same window, I said:\nNow transform the Stata code into a simple, easy to read format that’s in R code , make sure that it is very easy to understand and read. You must ensure that it is as simple to comprehend as possible.\nChatGPT did great here. Easy breezy, our histogram is now in R.\n\n\n4. Translate back to Stata\nGiven how well ChatGPT did at translating this code to R, and given how simple the code is now, I expected that this would be a breeze.\nBut it wasn’t, because once again ChatGPT showed a lack of understanding of Stata image formatting commands! When I told it the error I was getting, I once again got a very affirming response:\nYou’re absolutely right! In Stata, the edgecolor() option is not available for histograms. To correct that and ensure the code runs smoothly, I’ll remove that option and simplify the command while keeping it easy to understand.\n\n\n\nNot quite a breeze\n\n\nAfter this correction, the code did run, but ChatGPT forgot to add the frequency option, so the data y axis shows the probability of each outcome from 0 to 0.5. But the y axis labels are still set to reflect frequency. It’s not a good look!\n\n\n\n\nConclusion\nWhile ChatGPT was capable of performing the basic code translation tasks, it needed a lot of correction. It seemed to be a lot less comfortable working with Stata than with R. It’s true that R is open source and Stata isn’t, but I expected better results because Stata has been around for a long time. For reference, ChatGPT tells me that the first official version of R was released in February 2000, while Stata 1.0 was released in 1985.\nWhen pressed about its relative amounts of training data, ChatGPT did confess that it had more access to R data- though who knows if that is the truth.\n\n\nA final note: I had never before seen the Stata command that kept throwing me errors from the AI-generated responses. As far as I can tell, this command, edgecolor, is exclusively found in MATLAB syntax!"
  }
]